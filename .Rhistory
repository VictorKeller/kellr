View(df)
df$text[1:10]
table(df$text == " | ")
# ad number
n <- 0
for (i in 1:nrow(df)) {
df$ad <- n + 1
n <- ifelse(df$text == " | ", n + 1, n)
}
View(df)
# ad number
n <- 0
for (i in 1:nrow(df)) {
df$ad <- n + 1
if (df$text == " | ") {
n <- n + 1
}
}
# ad number
n <- 0
for (i in 1:nrow(df)) {
df$ad[i] <- n + 1
if (df$text[i] == " | ") {
n <- n + 1
}
}
"a" > 132
"a" < 132
# information
for (i in 1:nrow(df)) {
df$info[i] <- ifelse(grepl("km", df$text[i]), "km",
ifelse(grepl("Strada", df$text[i]) | grepl("Fiat", df$text[i]), "nome",
ifelse(grepl("R$", df$text[i]), "rs",
ifelse(is.numeric(df$text[i]) & df$text[i] > 1900 & df$text[i] < 2030, "ano",
NA))))
}
# information
for (i in 1:nrow(df)) {
df$info[i] <- ifelse(grepl("km", df$text[i]), "km",
ifelse(grepl("Strada", df$text[i]) | grepl("Fiat", df$text[i]), "nome",
ifelse(grepl("R$", df$text[i]), "rs",
ifelse(!is.na(as.numeric(df$text[i])) & df$text[i] > 1900 & df$text[i] < 2030, "ano",
NA))))
}
grepl("R$", df$text[i])
grepl("R$", df$text)
df$text
grepl("R$", df$text[364])
df$text[364]
grepl("0", df$text[364])
grepl("R", df$text[364])
grepl("R$", df$text[364])
grepl("R/$", df$text[364])
grepl("R//$", df$text[364])
grepl("R\$", df$text[364])
grepl("\$", df$text[364])
grepl("\\$", df$text[364])
grepl("R\\$", df$text[i])
grepl("R\\$", df$text[364])
# information
for (i in 1:nrow(df)) {
df$info[i] <- ifelse(grepl("km", df$text[i]), "km",
ifelse(grepl("Strada", df$text[i]) | grepl("Fiat", df$text[i]), "nome",
ifelse(grepl("R\\$", df$text[i]), "rs",
ifelse(!is.na(as.numeric(df$text[i])) & df$text[i] > 1900 & df$text[i] < 2030, "ano",
NA))))
}
ifelse(T & NA, 1, 2)
nchar(df$text[i])
# information
for (i in 1:nrow(df)) {
df$info[i] <- ifelse(grepl("km", df$text[i]), "km",
ifelse(grepl("Strada", df$text[i]) | grepl("Fiat", df$text[i]), "nome",
ifelse(grepl("R\\$", df$text[i]), "rs",
ifelse(!is.na(as.numeric(df$text[i])) & nchar(df$text[i]) == 4, "ano",
NA))))
}
# Load packages and functions ####
library(tidyverse)
library(AnthroTools)
names_with <- function(df, x) names(df)[grepl(x, names(df))]
clist <- function(x, sep = ", ") strsplit(x, sep)[[1]]
# Load data ####
resp.fem <- read.csv("C:/Users/VNFSK/Dropbox/Consultoria/Mariana/femininity_utf.csv", stringsAsFactors = F, header = F)
resp.masc <- read.csv("C:/Users/VNFSK/Dropbox/Consultoria/Mariana/Masculinities_utf.csv", stringsAsFactors = F, header = F)
syn.fem <- read.csv("C:/Users/VNFSK/Dropbox/Consultoria/Mariana/Synonyms female_utf.csv", stringsAsFactors = F, header = T)
syn.masc <- read.csv("C:/Users/VNFSK/Dropbox/Consultoria/Mariana/Synonyms male_utf.csv", stringsAsFactors = F, header = T)
# Fix variable names ####
# Store variable names
varnames <- paste(resp.fem[1, ])
# Substitute large names with codes
names(resp.fem) <- c("ResponseID", names(resp.fem)[2:13], varnames[14:89])
names(resp.masc) <- c("ResponseID", names(resp.masc)[2:13], varnames[14:89])
# Remove 1st row (because it only has the variable names)
resp.fem <- resp.fem[2:nrow(resp.fem), ]
resp.masc <- resp.fem[2:nrow(resp.masc), ]
# Restructure synonym data ####
syn.fem1 <- NULL
for (i in 1:ncol(syn.fem)) {
temp <- data.frame(syns = syn.fem[, i]) #store ith column from synonym data in temporary data frame
temp[["keyword"]] <- temp$syns[1] #create a column indicating the keyword for that synonym
temp <- temp[2:nrow(temp), ] #remove 1st row because it had the keyword as a synonym
temp <- temp[temp$syns != "", ] #remove rows with missing values
syn.fem1 <- rbind(syn.fem1, temp) #add temp to syn.fem1
}
length(unique(syn.fem1$keyword)) #how many unique keywords does syn.fem1 have?
syn.fem <- unique(syn.fem1)
syn.masc1 <- NULL
for (i in 1:ncol(syn.masc)) {
temp <- data.frame(syns = syn.masc[, i]) #store ith column from synonym data in temporary data frame
temp[["keyword"]] <- temp$syns[1] #create a column indicating the keyword for that synonym
temp <- temp[2:nrow(temp), ] #remove 1st row because it had the keyword as a synonym
temp <- temp[temp$syns != "", ] #remove rows with missing values
syn.masc1 <- rbind(syn.masc1, temp) #add temp to syn.masc1
}
length(unique(syn.masc1$keyword)) #how many unique keywords does syn.masc1 have?
syn.masc <- unique(syn.masc1)
# PROBLEM: Repeated masculine synonyms ####
length(unique(syn.fem$syns))
length(unique(syn.masc$syns))
# Check which are repeated
temp <- syn.masc %>%
group_by(syns) %>%
mutate(n = n()) %>%
arrange(desc(n))
View(temp)
# Solution for now: only use first repeat
repeats <- temp[temp$n > 1, ] %>%
group_by(syns) %>%
slice(2:nrow(.))
syn.masc <- syn.masc[!(syn.masc$syns %in% repeats$syns & syn.masc$keyword %in% repeats$keyword), ]
# Substitute synonyms in response data for keywords ####
#first check how many unique words
temp <- NULL
for(i in 14:ncol(resp.fem)) {
temp <- c(temp, resp.fem[[i]])
}
length(temp[temp != ""]) #total number of non-empty responses
length(unique(temp)) #number of unique responses
for (j in 14:ncol(resp.fem)) {
for (i in 1:nrow(resp.fem)) {
resp.fem[i, j] <- ifelse(tolower(resp.fem[i, j]) %in% tolower(syn.fem$syns), #if response is in synonym list
syn.fem$keyword[tolower(syn.fem$syns) == tolower(resp.fem[i, j])], #substitute for keyword
resp.fem[i, j]) #if not, leave as is
}
}
#check again, should have changed unique resps
temp <- NULL
for(i in 14:ncol(resp.fem)) {
temp <- c(temp, resp.fem[[i]])
}
length(temp[temp != ""]) #total number of non-empty responses
length(unique(temp)) #number of unique responses
#same for male
#first check how many unique words
temp <- NULL
for(i in 14:ncol(resp.masc)) {
temp <- c(temp, resp.masc[[i]])
}
length(temp[temp != ""]) #total number of non-empty responses
length(unique(temp)) #number of unique responses
for (j in 14:ncol(resp.masc)) {
for (i in 1:nrow(resp.masc)) {
resp.masc[i, j] <- ifelse(tolower(resp.masc[i, j]) %in% tolower(syn.masc$syns), #if response is in synonym list
syn.masc$keyword[tolower(syn.masc$syns) == tolower(resp.masc[i, j])], #substitute for keyword
resp.masc[i, j]) #if not, leave as is
}
}
#check again, should have changed unique resps
temp <- NULL
for(i in 14:ncol(resp.masc)) {
temp <- c(temp, resp.masc[[i]])
}
length(temp[temp != ""]) #total number of non-empty responses
length(unique(temp)) #number of unique responses
# Restructure response data so that we can calculate salience ####
fqs <- list()
for (q in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) { #for each question/prompt
fqs[[q]] <- resp.fem[, c("ResponseID", names_with(resp.fem, q))] %>% #create a data frame with responseID and the answers for that q
gather(key = "ques", value = "resp", names_with(resp.fem, q)) #restructure the data frame so that it's "long" (one line per answer)
fqs[[q]]$question <- q #create column indicating which question
fqs[[q]]$resp.order <- gsub(q, "", fqs[[q]]$ques) #create column indicating response order
fqs[[q]] <- select(fqs[[q]], -ques) %>% #remove column and
filter(resp != "") #remove empty responses
}
mqs <- list()
for (q in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) { #for each question/prompt
mqs[[q]] <- resp.masc[, c("ResponseID", names_with(resp.masc, q))] %>% #create a data frame with responseID and the answers for that q
gather(key = "ques", value = "resp", names_with(resp.masc, q)) #restructure the data frame so that it's "long" (one line per answer)
mqs[[q]]$question <- q #create column indicating which question
mqs[[q]]$resp.order <- gsub(q, "", mqs[[q]]$ques) #create column indicating response order
mqs[[q]] <- select(mqs[[q]], -ques) %>% #remove column and
filter(resp != "") #remove empty responses
}
# Calculate term salience in list (grouping by prompt) -- percentile rank ####
for (q in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
fqs[[q]]$ResponseID <- as.numeric(fqs[[q]]$ResponseID) #transforms to numeric
fqs[[q]]$resp.order <- as.numeric(fqs[[q]]$resp.order)
fqs[[q]] <- CalculateSalience(mydata = fqs[[q]], Subj = "ResponseID", Order = "resp.order", CODE = "resp")
}
for (q in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
mqs[[q]]$ResponseID <- as.numeric(mqs[[q]]$ResponseID) #transforms to numeric
mqs[[q]]$resp.order <- as.numeric(mqs[[q]]$resp.order)
mqs[[q]] <- CalculateSalience(mydata = mqs[[q]], Subj = "ResponseID", Order = "resp.order", CODE = "resp")
}
# Calculate mean salience for all the terms ####
fts <- list() #this will be the list of data frames for femininity term salience (fts)
for (q in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
n.lists <- length(unique(mqs[[q]]$ResponseID))
fts[[q]] <- fqs[[q]] %>%
mutate(resp = tolower(resp)) %>%
group_by(resp) %>%
summarise(meanSalience = sum(Salience, na.rm = T) / n.lists, #mean salience
n.mentions = n(), #number of times it was mentioned
prevalence = n() / n.lists) #prevalence
}
mts <- list() #this will be the list of data frames for masculinity term salience (mts)
for (q in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
n.lists <- length(unique(mqs[[q]]$ResponseID))
mts[[q]] <- mqs[[q]] %>%
mutate(resp = tolower(resp)) %>%
group_by(resp) %>%
summarise(meanSalience = sum(Salience, na.rm = T) / n.lists, #mean salience
n.mentions = n(), #number of times it was mentioned
prevalence = n() / n.lists) #prevalence
}
# Saturation:
# less than 1 new item per additional participant
# model number of new items per additional participant and see when that is lower than 1?
# probably never because the domain size is VERY large...
# saturation in salience: are you capturing the most salient ideas? they defined which ideas were salient (prevalence > 20%) and
#check how many people are needed to capture those ideas. With extensive prompting, saturation in salience is easy
# Information on participants:
#mean number of responses per prompt
fpart <- list()
for (q in names(fqs)) {
fpart[[q]] <- fqs[[q]] %>%
group_by(ResponseID) %>%
summarise(n.resp = n(),
n.resp.unique = length(unique(resp)),
n.repeats = n.resp - n.resp.unique)
}
mpart <- list()
for (q in names(mqs)) {
mpart[[q]] <- mqs[[q]] %>%
group_by(ResponseID) %>%
summarise(n.resp = n(),
n.resp.unique = length(unique(resp)),
n.repeats = n.resp - n.resp.unique)
}
fpart[[q]]
fpart <- list()
for (q in names(fqs)) {
fpart[[q]] <- fqs[[q]] %>%
group_by(ResponseID) %>%
summarise(n.resp = n(), #number of responses
n.resp.unique = length(unique(resp)), #number of unique responses (some may have been repeats after sub-ing the synonyms)
n.repeats = n.resp - n.resp.unique) #number of repeats
mean(fpart[[q]]$n.resp, na.rm = T)
table(fpart[[q]]$n.resp)
mean(fpart[[q]]$n.resp.unique, na.rm = T)
table(fpart[[q]]$n.resp.unique)
}
fpart <- list()
for (q in names(fqs)) {
fpart[[q]] <- fqs[[q]] %>%
group_by(ResponseID) %>%
summarise(n.resp = n(), #number of responses
n.resp.unique = length(unique(resp)), #number of unique responses (some may have been repeats after sub-ing the synonyms)
n.repeats = n.resp - n.resp.unique) #number of repeats
print(mean(fpart[[q]]$n.resp, na.rm = T),
table(fpart[[q]]$n.resp),
mean(fpart[[q]]$n.resp.unique, na.rm = T),
table(fpart[[q]]$n.resp.unique))
}
fpart <- list()
for (q in names(fqs)) {
fpart[[q]] <- fqs[[q]] %>%
group_by(ResponseID) %>%
summarise(n.resp = n(), #number of responses
n.resp.unique = length(unique(resp)), #number of unique responses (some may have been repeats after sub-ing the synonyms)
n.repeats = n.resp - n.resp.unique) #number of repeats
print(mean(fpart[[q]]$n.resp, na.rm = T))
print(table(fpart[[q]]$n.resp))
print(mean(fpart[[q]]$n.resp.unique, na.rm = T))
print(table(fpart[[q]]$n.resp.unique))
}
fpart <- list()
for (q in names(fqs)) {
fpart[[q]] <- fqs[[q]] %>%
group_by(ResponseID) %>%
summarise(n.resp = n(), #number of responses
n.resp.unique = length(unique(resp)), #number of unique responses (some may have been repeats after sub-ing the synonyms)
n.repeats = n.resp - n.resp.unique) #number of repeats
print(q)
print(mean(fpart[[q]]$n.resp, na.rm = T))
print(table(fpart[[q]]$n.resp))
print(mean(fpart[[q]]$n.resp.unique, na.rm = T))
print(table(fpart[[q]]$n.resp.unique))
}
fpart <- list()
for (q in names(fqs)) {
fpart[[q]] <- fqs[[q]] %>%
group_by(ResponseID) %>%
summarise(n.resp = n(), #number of responses
n.resp.unique = length(unique(resp)), #number of unique responses (some may have been repeats after sub-ing the synonyms)
n.repeats = n.resp - n.resp.unique) #number of repeats
print(q)
print(mean(fpart[[q]]$n.resp, na.rm = T))
print(table(fpart[[q]]$n.resp))
print(mean(fpart[[q]]$n.repeats, na.rm = T))
print(table(fpart[[q]]$n.repeats))
}
fqs[[q]]
fts[[q]]
for (q in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
fqs[[q]]$resp.novel <- ifelse(fqs[[q]]$resp %in% fts[[q]]$resp[fts[[q]]$n.mentions == 1],
"yes",
"no")
}
fqs[[q]]
View(fts[[q]])
for (q in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
fqs[[q]]$resp.novel <- ifelse(tolower(fqs[[q]]$resp) %in% tolower(fts[[q]]$resp[fts[[q]]$n.mentions == 1]),
"yes",
"no")
}
View(fts[[q]])
q
tolower(fts[[q]]$resp[fts[[q]]$n.mentions == 1]
)
tolower(fqs[[q]]$resp)
"doesnÂ´t work too much" %in% tolower(fqs[[q]]$resp)
tolower(fqs[[q]]$resp)
"doesn't work too much" %in% tolower(fqs[[q]]$resp)
tolower(fqs[[q]]$resp)[155]
tolower(fqs[[q]]$resp)[155] == "doesn't work too much"
View(fts[[q]])
View(fqs[[q]])
fqs[[q]]
View(fts[[q]])
for (q in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
mqs[[q]]$resp.novel <- ifelse(tolower(mqs[[q]]$resp) %in% tolower(mts[[q]]$resp[mts[[q]]$n.mentions == 1]),
"yes",
"no")
}
View(mqs[[q]])
View(mts[[q]])
fpart <- list()
for (q in names(fqs)) {
fpart[[q]] <- fqs[[q]] %>%
group_by(ResponseID) %>%
summarise(n.resp = n(), #number of responses
n.resp.unique = length(unique(resp)), #number of unique responses (some may have been repeats after sub-ing the synonyms)
n.repeats = n.resp - n.resp.unique, #number of repeats
n.novel = sum(resp.novel == "yes", na.rm = T))
}
fpart[[q]]
View(fpart[[q]])
mpart <- list()
for (q in names(mqs)) {
mpart[[q]] <- mqs[[q]] %>%
group_by(ResponseID) %>%
summarise(n.resp = n(),
n.resp.unique = length(unique(resp)),
n.repeats = n.resp - n.resp.unique,
n.novel = sum(resp.novel == "yes", na.rm = T))
}
View(mpart[[q]])
q
View(mqs[[q]])
mean(fpart[[q]]$n.novel)
mean(fpart[[q]]$n.resp)
fts[[q]]
f.part.prompts <- list()
for (i in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
f.part.prompts[[q]] <- fpart %>%
summarise(
mean.n.part.resp = mean(n.resp, na.rm = T), #mean number of responses per participant
mean.n.part.novel = mean(n.novel, na.rm = T) #mean number of novel responses per participant
) %>%
mutate(prompt = q)
}
f.part.prompts <- list()
for (i in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
f.part.prompts[[q]] <- fpart[[q]] %>%
summarise(
mean.n.part.resp = mean(n.resp, na.rm = T), #mean number of responses per participant
mean.n.part.novel = mean(n.novel, na.rm = T) #mean number of novel responses per participant
) %>%
mutate(prompt = q)
}
f.part.prompts[[q]]
f.resp.prompts <- list()
for (i in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
f.resp.prompts[[q]] <- fts[[q]] %>%
summarise(
total.n.terms = n(), #total number of terms mentioned
mean.salience = mean(meanSalience, na.rm = T), #mean salience of all the terms
n.terms.novel = sum(n.mentions == 1, na.rm = T), #number of terms that are novel
pct.terms.novel = round((sum(n.mentions == 1, na.rm = T) / n()) * 100, 1), #percentage of terms that are novel (ie terms only mentioned once)
n.terms.prev20 = sum(prevalence > .2, na.rm = T), #number of terms that have prevalence > 20%
pct.terms.prev20 = round((sum(prevalence > .2, na.rm = T) / n()) * 100, 1) #percentage of terms that have prevalence > 20%
) %>%
mutate(prompt = q)
}
f.resp.prompts[[q]]
f.part.prompts <- list()
for (i in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
f.part.prompts[[q]] <- fpart[[q]] %>%
summarise(
mean.n.part.resp = mean(n.resp, na.rm = T), #mean number of responses per participant
mean.n.part.novel = mean(n.novel, na.rm = T) #mean number of novel responses per participant
) %>%
mutate(prompt = q)
}
f.part.prompts <- reduce(f.part.prompts, rbind)
f.resp.prompts <- list()
for (i in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
f.resp.prompts[[q]] <- fts[[q]] %>%
summarise(
total.n.terms = n(), #total number of terms mentioned
mean.salience = mean(meanSalience, na.rm = T), #mean salience of all the terms
n.terms.novel = sum(n.mentions == 1, na.rm = T), #number of terms that are novel
pct.terms.novel = round((sum(n.mentions == 1, na.rm = T) / n()) * 100, 1), #percentage of terms that are novel (ie terms only mentioned once)
n.terms.prev20 = sum(prevalence > .2, na.rm = T), #number of terms that have prevalence > 20%
pct.terms.prev20 = round((sum(prevalence > .2, na.rm = T) / n()) * 100, 1) #percentage of terms that have prevalence > 20%
) %>%
mutate(prompt = q)
}
f.resp.prompts <- reduce(f.resp.prompts, rbind)
f.part.prompts
f.resp.prompts
f.part.prompts <- list()
for (i in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
f.part.prompts[[q]] <- fpart[[q]] %>%
summarise(
mean.n.part.resp = mean(n.resp, na.rm = T), #mean number of responses per participant
mean.n.part.novel = mean(n.novel, na.rm = T) #mean number of novel responses per participant
) %>%
mutate(prompt = q)
}
# f.part.prompts <- reduce(f.part.prompts, rbind)
f.resp.prompts <- list()
for (i in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
f.resp.prompts[[q]] <- fts[[q]] %>%
summarise(
total.n.terms = n(), #total number of terms mentioned
mean.salience = mean(meanSalience, na.rm = T), #mean salience of all the terms
n.terms.novel = sum(n.mentions == 1, na.rm = T), #number of terms that are novel
pct.terms.novel = round((sum(n.mentions == 1, na.rm = T) / n()) * 100, 1), #percentage of terms that are novel (ie terms only mentioned once)
n.terms.prev20 = sum(prevalence > .2, na.rm = T), #number of terms that have prevalence > 20%
pct.terms.prev20 = round((sum(prevalence > .2, na.rm = T) / n()) * 100, 1) #percentage of terms that have prevalence > 20%
) %>%
mutate(prompt = q)
}
View(f.part.prompts)
# Information on prompts
f.part.prompts <- list()
for (q in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
f.part.prompts[[q]] <- fpart[[q]] %>%
summarise(
mean.n.part.resp = mean(n.resp, na.rm = T), #mean number of responses per participant
mean.n.part.novel = mean(n.novel, na.rm = T) #mean number of novel responses per participant
) %>%
mutate(prompt = q)
}
f.part.prompts <- reduce(f.part.prompts, rbind)
f.resp.prompts <- list()
for (q in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
f.resp.prompts[[q]] <- fts[[q]] %>%
summarise(
total.n.terms = n(), #total number of terms mentioned
mean.salience = mean(meanSalience, na.rm = T), #mean salience of all the terms
n.terms.novel = sum(n.mentions == 1, na.rm = T), #number of terms that are novel
pct.terms.novel = round((sum(n.mentions == 1, na.rm = T) / n()) * 100, 1), #percentage of terms that are novel (ie terms only mentioned once)
n.terms.prev20 = sum(prevalence > .2, na.rm = T), #number of terms that have prevalence > 20%
pct.terms.prev20 = round((sum(prevalence > .2, na.rm = T) / n()) * 100, 1) #percentage of terms that have prevalence > 20%
) %>%
mutate(prompt = q)
}
f.resp.prompts <- reduce(f.resp.prompts, rbind)
View(f.part.prompts)
View(f.resp.prompts)
fem.prompts <- merge(f.part.prompts, f.resp.prompts)
View(fem.prompts)
m.part.prompts <- list()
for (q in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
m.part.prompts[[q]] <- mpart[[q]] %>%
summarise(
mean.n.part.resp = mean(n.resp, na.rm = T), #mean number of responses per participant
mean.n.part.novel = mean(n.novel, na.rm = T) #mean number of novel responses per participant
) %>%
mutate(prompt = q)
}
m.part.prompts <- reduce(m.part.prompts, rbind)
m.resp.prompts <- list()
for (q in clist("MLGW, CSW, WNFC, PQDW, GWS, GWL, GM")) {
m.resp.prompts[[q]] <- mts[[q]] %>%
summarise(
mean.salience = mean(meanSalience, na.rm = T), #mean salience of all the terms
total.n.terms = n(), #total number of terms mentioned
n.terms.novel = sum(n.mentions == 1, na.rm = T), #number of terms that are novel
pct.terms.novel = round((sum(n.mentions == 1, na.rm = T) / n()) * 100, 1), #percentage of terms that are novel (ie terms only mentioned once)
n.terms.prev20 = sum(prevalence > .2, na.rm = T), #number of terms that have prevalence > 20%
pct.terms.prev20 = round((sum(prevalence > .2, na.rm = T) / n()) * 100, 1) #percentage of terms that have prevalence > 20%
) %>%
mutate(prompt = q)
}
m.resp.prompts <- reduce(m.resp.prompts, rbind)
masc.prompts <- merge(m.part.prompts, m.resp.prompts)
View(masc.prompts)
